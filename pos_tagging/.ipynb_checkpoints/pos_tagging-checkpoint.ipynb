{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "data = pd.read_csv('../../Datasets/yelp_labelled_processed/yelp_labelled_processed.csv')\n",
    "\n",
    "# Replace non-string reviews with strings (this is jsut a quirck of this dataset becasue some are np.nan)\n",
    "data['text'] = data['text'].apply(lambda review: str(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert string to array of tokens\n",
    "The nltk pos tagger accepts a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each string into an array\n",
    "data['tokens'] = data['text'].copy().apply(lambda string: string.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [new, rule, waitingtable, almostalways, cant, ...\n",
       "1    [giving, twostar, 'spretty, rating, might, nig...\n",
       "2    [staying, planet, hollywood, acrossstreet, saw...\n",
       "3    [foodgood, price, super, expensive, 8, buck, e...\n",
       "4    [worse, company, deal, horrible, work, bring, ...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the first 5 rows\n",
    "data['tokens'].head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply POS tagging\n",
    "data['nltk_pos_tagged'] = data['tokens'].apply(lambda tokens: nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 'JJ'), ('rule', 'NN'), ('waitingtable', 'JJ'), ('almostalways', 'NNS'), ('cant', 'VBP'), ('wait', 'NN'), ('inside', 'RB'), ('posted', 'VBD'), ('sign', 'JJ'), ('upfront', 'JJ'), ('cause', 'NN'), ('concern', 'NN'), ('seated', 'VBN'), ('patron', 'RB'), ('awful', 'JJ'), ('like', 'IN'), ('included', 'JJ'), ('apology', 'NN'), ('along', 'IN'), ('especially', 'RB'), ('cold', 'JJ'), ('p.s', 'JJ'), ('try', 'NN'), ('calling', 'VBG'), ('ahead', 'RB'), ('reserve', 'NN'), ('table', 'JJ'), ('thats', 'NNS'), ('waiting', 'VBG'), ('list', 'NN'), ('short', 'JJ'), ('otherwise', 'RB'), ('show', 'VBP'), ('reserve', 'NN'), ('placecould', 'NN'), ('wrong', 'JJ'), ('eye', 'NN'), ('rattle', 'VB'), ('away', 'RP'), ('hot', 'JJ'), ('beverage', 'NN'), ('must', 'MD'), ('mention', 'VB'), ('obsessed', 'VBD'), ('mad', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first text\n",
    "print(data['nltk_pos_tagged'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get information for POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "# Get information of tag\n",
    "nltk.help.upenn_tagset(\"NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy's 'en_core_web_sm' model\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No need to convert from string to array of tokens\n",
    "The spacy pos tagger accepts strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply POS tagging\n",
    "data['spacy_pos_tagged'] = data['text'].apply(lambda tokens: nlp(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new ADJ JJ\n",
      "rule NOUN NN\n",
      "waitingtable ADJ JJ\n",
      "almostalways PROPN NNP\n",
      "ca VERB MD\n",
      "nt PART RB\n",
      "wait VERB VB\n",
      "inside ADV RB\n",
      "posted VERB VBN\n",
      "sign NOUN NN\n",
      "upfront NOUN NN\n",
      "cause NOUN NN\n",
      "concern NOUN NN\n",
      "seated VERB VBD\n",
      "patron NOUN NN\n",
      "awful ADJ JJ\n",
      "like SCONJ IN\n",
      "included VERB VBN\n",
      "apology NOUN NN\n",
      "along ADP IN\n",
      "especially ADV RB\n",
      "cold ADJ JJ\n",
      "p.s PROPN NNP\n",
      "try VERB VBP\n",
      "calling VERB VBG\n",
      "ahead ADV RB\n",
      "reserve NOUN NN\n",
      "table NOUN NN\n",
      "that DET WDT\n",
      "s VERB VBZ\n",
      "waiting VERB VBG\n",
      "list NOUN NN\n",
      "short PROPN NNP\n",
      "otherwise ADV RB\n",
      "show VERB VBP\n",
      "reserve PROPN NNP\n",
      "placecould PROPN NNP\n",
      "wrong PROPN NNP\n",
      "eye NOUN NN\n",
      "rattle VERB VB\n",
      "away ADV RB\n",
      "hot ADJ JJ\n",
      "beverage NOUN NN\n",
      "must VERB MD\n",
      "mention VERB VB\n",
      "obsessed VERB VBN\n",
      "mad ADJ JJ\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first text\n",
    "first_text = data['spacy_pos_tagged'][0]\n",
    "for token in first_text:\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get information for POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'verb, 3rd person singular present'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get information of tag\n",
    "spacy.explain(\"VBZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your own POS tagger\n",
    "The first thing to do is pick a corpus that we want to train our tagger on. Import the necessary Python packages. Here, we use the nltk treebank corpus to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('treebank')\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged sentences:  3914\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged words: 100676\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capitals_inside': False,\n",
      " 'has_hyphen': False,\n",
      " 'is_all_caps': False,\n",
      " 'is_all_lower': True,\n",
      " 'is_capitalized': False,\n",
      " 'is_first': False,\n",
      " 'is_last': False,\n",
      " 'is_numeric': False,\n",
      " 'next_word': 'sentence',\n",
      " 'prefix-1': 'a',\n",
      " 'prefix-2': 'a',\n",
      " 'prefix-3': 'a',\n",
      " 'prev_word': 'is',\n",
      " 'suffix-1': 'a',\n",
      " 'suffix-2': 'a',\n",
      " 'suffix-3': 'a',\n",
      " 'word': 'a'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(features(['This', 'is', 'a', 'sentence'], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to strip the tagged words of their tags so that we can feed them into our tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to build our training set. Our tagger needs to take features individually for each word, but our corpus is actually in the form of sentences, so we need to do a little transforming. Split the data into training and testing sets. Apply this function on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
