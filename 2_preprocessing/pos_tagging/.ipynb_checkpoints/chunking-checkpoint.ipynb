{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "data = pd.read_csv('../../../Datasets/yelp_labelled_processed/yelp_labelled_processed.csv')[0:100]\n",
    "\n",
    "# Replace non-string reviews with strings (this is jsut a quirck of this dataset becasue some are np.nan)\n",
    "data['text'] = data['text'].apply(lambda review: str(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply chunking with nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert string to array of tokens\n",
    "The nltk pos tagger accepts a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each string into an array\n",
    "data['tokens'] = data['text'].copy().apply(lambda string: string.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply POS tagging\n",
    "data['nltk_pos_tagged'] = data['tokens'].apply(lambda tokens: nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunk parser present within the NLTK library is rule based and thus needs to be given a regular expression as a rule to output a chunk with its chunk tag. i.e It requires an a regular expression of a phrase and the corresponding chunk tag. It then searches the corpus for this expression and assigns it the tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regular expression that will search for a noun phrase, as shown:\n",
    "rule = r\"\"\"Noun Phrase: {<DT>?<JJ>*<NN>}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regular expression is searching for a determiner (optional), followed by one or more adjectives and then a single noun. This will form a chunk called Noun Phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of RegexpParser and feed it the rule:\n",
    "chunkParser = nltk.RegexpParser(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give chunkParser the tagset containing the tokens with their respective POS tags so that it can perform chunking, and then draw the chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply chunking\n",
    "data['nltk_chunked'] = data['nltk_pos_tagged'].apply(lambda tagset: chunkParser.parse(tagset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look at the chunked phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a parse tree (This will open a new window)\n",
    "data['nltk_chunked'][0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[(new, JJ), (rule, NN)], (waitingtable, JJ), ...\n",
       "1    [(giving, VBG), [(twostar, NN)], ('spretty, CD...\n",
       "2    [(staying, VBG), [(planet, NN)], [(hollywood, ...\n",
       "3    [[(foodgood, NN)], [(price, NN)], (super, VBD)...\n",
       "4    [(worse, JJR), [(company, NN)], [(deal, NN)], ...\n",
       "Name: nltk_chunked, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the first 5 rows\n",
    "data['nltk_chunked'].head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply chunking with spacy\n",
    "spaCy doesn't require us to formulate rules to recognize chunks; it identifies chunks on its own and tells us what the head word is, thus telling us what the chunk tag is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy's 'en_core_web_sm' model\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No need to convert from string to array of tokens\n",
    "The spacy pos tagger accepts strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply POS tagging\n",
    "data['spacy_pos_tagged'] = data['text'].apply(lambda tokens: nlp(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the chunker\n",
    "Apply noun_chunks on this model, and for each chunk, print the text of the chunk, the root word of the chunk, and the dependency relation that connects the root word to its head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new rule rule ROOT\n",
      "waitingtable almostalways almostalways nsubj\n",
      "posted sign upfront cause concern concern nsubj\n",
      "patron patron dobj\n",
      "apology apology dobj\n",
      "reserve table table dobj\n",
      "list short otherwise show reserve reserve dobj\n",
      "placecould placecould ROOT\n",
      "hot beverage beverage nsubj\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first text\n",
    "first_text = data['spacy_pos_tagged'][0]\n",
    "for chunk in first_text.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
