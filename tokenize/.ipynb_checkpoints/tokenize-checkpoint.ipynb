{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Import from nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams, FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "data = pd.read_csv('../../Datasets/yelp_labelled_processed/yelp_labelled_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The new rule is - \\r\\nif you are waiting for a...\n",
       "1    Flirted with giving this two stars, but that's...\n",
       "2    I was staying at planet Hollywood across the s...\n",
       "3    Food is good but prices are super expensive.  ...\n",
       "4    Worse company to deal with they do horrible wo...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the first 5 tests\n",
    "data['text'].head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nltk - the old fashioned way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stop words\n",
    "stop_words = stopwords.words('english') # NLTK stop_words\n",
    "\n",
    "# Remove certain words from the list of stop words\n",
    "words_to_remove = [\"not\", \"too\", \"very\", \"don\", \"don't\", \"should\", \"should've\", \"aren\", \"aren't\", \n",
    "                   \"couldn\", \"couldn't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \"hadn\", \"hadn't\", \n",
    "                   \"hasn\", \"hasn't\", \"haven\", \"haven't\", \"isn\", \"isn't\", \"shouldn\", \"shouldn't\", \n",
    "                   \"wasn\", \"wasn't\", \"weren\", \"weren't\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \n",
    "                   \"shan\", \"shan't\", \"mightn\", \"mightn't\", \"mustn\", \"mustn't\", \"needn\", \"needn't\"]\n",
    "for word in words_to_remove:\n",
    "    stop_words.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the lemmantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer\n",
    "def tokenize(review):\n",
    "    review = review.lower() # make everything lower case\n",
    "    tokens = word_tokenize(review) # use NLTK tokenizer to generate tokens\n",
    "    tokens = [token for token in tokens if bool(re.search(r'\\w{1,}', token))] # remove tokens that don't have letters or numbers in them\n",
    "    tokens = [token for token in tokens if not token in stop_words] # remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # return each token in their base form\n",
    "    \n",
    "    return ' '.join(tokens) # return the review tokenized review as a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the reviews\n",
    "data['text'] = data['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    new rule waiting table almost always cant wait...\n",
       "1    flirted giving two star 's pretty damning rati...\n",
       "2    staying planet hollywood across street saw goo...\n",
       "3    food good price super expensive 8 buck extra l...\n",
       "4    worse company deal horrible work bring truck b...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the first 5 tests\n",
    "data['text'].head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
